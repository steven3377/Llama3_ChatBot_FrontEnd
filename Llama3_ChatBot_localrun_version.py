import streamlit as st
import os
import requests
import json
import speech_recognition as sr  # æ–°å¢çš„èªéŸ³è¾¨è­˜åº«
from Llama3_Backend_API_Url import SFT_LLAMA3_8B_MODEL_PATH, SFT_LLAMA2_7B_MODEL_PATH, META_LLAMA2_7B_MODEL_PATH

# Load the entire model on the GPU 0 or Auto #
device_map = "auto" #{"": 0}

global chat_model, tokenizer, voice_prompt

temperature = 0.1
top_p = 0.9
max_length = 200
voice_prompt = " "

# App title
st.set_page_config(page_title="ğŸ¦™ğŸ’¬ Llama Chatbot-éˆå‹•æ™ºä¼´")


#system_prompt = st.sidebar.text_input('System prompt', "ä½ æ˜¯ä¸€å€‹æœ‰å¹«åŠ©çš„å•ç­”æ©Ÿå™¨äººï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›è¦†ã€‚")
system_prompt = "ä½ æ˜¯ä¸€å€‹å…·æœ‰å‹å·¥çŸ¥è­˜åŠå¾ˆæœ‰æ™ºæ…§èˆ‡å¹«åŠ©çš„å•ç­”æ©Ÿå™¨äººï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"

def response_api(prompt, url, system_prompt):
    # payload = json.dumps({
    #   "messages": [
    #     {
    #       "role": "system",
    #       "content": system_prompt
    #     },
    #     {
    #       "role": "user",
    #       "content": prompt
    #     }
    #   ]
    # })

    payload = json.dumps({
      "messages": system_prompt + prompt,
      "temperature": temperature,
      "maxlength": max_length
    })
    headers = {
       'Content-Type': 'application/json'
    }

    response = requests.request("POST", url, headers=headers, data=payload)

    print("JSON:", response.json())  # Steven test 2023/10/14 #

    #return response.json()["choices"][0]["message"]['content']
    return response.json()

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

st.header("<My Great ChatBot ğŸ¤—>")

# Replicate Credentials
with st.sidebar:
    st.title('ğŸ¦™ğŸ¦™ğŸ’¬ éˆå‹•æ™ºä¼´\n(Llama 3.1 Chatbot)')

    st.subheader('Models and parameters')
    selected_model = st.sidebar.selectbox('Choose a Llama model', ['SFT_Llama3-8B', 'SFT_Llama2-7B', 'Meta_Llama2-7B'], key='selected_model')   
    if selected_model == 'SFT_Llama3-8B':
        api_url = SFT_LLAMA3_8B_MODEL_PATH  # [Backend URL] Change here... #
    elif selected_model == 'SFT_Llama2-7B':
        api_url = SFT_LLAMA2_7B_MODEL_PATH  # [Backend URL] Change here... #
    elif selected_model == 'Meta_Llama2-7B':
        api_url = META_LLAMA2_7B_MODEL_PATH  # [Backend URL] Change here... #

    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()
    temperature = st.sidebar.slider('æº«åº¦ä¿‚æ•¸(temperature)', min_value=0.0, max_value=3.0, value=0.3, step=0.1)
    top_p = st.sidebar.slider('æ ¸å–æ¨£(top_p)', min_value=0.01, max_value=1.0, value=0.9, step=0.01)
    max_length = st.sidebar.slider('å›è¦†é•·åº¦(max_length)', min_value=128, max_value=512, value=300, step=8)
    st.sidebar.button('æ¸…é™¤å°è©±è¨˜éŒ„\n(Clear Chat History)', on_click=clear_chat_history)
    st.markdown('ğŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')
    

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "å“ˆå›‰ï¼ä»Šå¤©åˆæ˜¯ä¸€å€‹å­¸ç¿’AIçš„å¥½æ—¥å­ã€‚"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# User-provided prompt
prompt = st.chat_input("è«‹è¼¸å…¥è¨Šæ¯...")
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Add a button for speech recognition
if st.button("èªéŸ³è¾¨è­˜è¼¸å…¥"):
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("è«‹é–‹å§‹èªªè©±...")
        with st.spinner("Listening ..."):
            audio = recognizer.listen(source, timeout=2)

    try:
        # ä½¿ç”¨Google WebèªéŸ³è¾¨è­˜æœå‹™é€²è¡ŒèªéŸ³è½‰æ–‡å­—
        recognized_text = recognizer.recognize_google(audio, language="zh-TW")
        # å°‡èªéŸ³è¾¨è­˜çš„å…§å®¹è¨­å®šç‚ºæ–‡æœ¬è¼¸å…¥æ¡†çš„å…§å®¹
        #st.session_state.messages.append({"role": "user", "content": recognized_text})
        #with st.chat_message("user"):
        #    st.write(recognized_text)
        voice_prompt = recognized_text
    except sr.WaitTimeoutError:
        st.write("èªéŸ³è¾¨è­˜è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚")
    except sr.UnknownValueError:
        st.write("ç„¡æ³•è­˜åˆ¥èªéŸ³ã€‚")
    except sr.RequestError as e:
        st.write("èªéŸ³è¾¨è­˜æœå‹™éŒ¯èª¤ï¼š {0}".format(e))

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = response_api(prompt, api_url, system_prompt)
            placeholder = st.empty()
            full_response = ''
            for item in response:
                full_response += item
                placeholder.markdown(full_response)
            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)

# Voice-Text auto-filled solution - Steven 2023/10/13 #
#default_chat_input_value = "Default Value"
default_chat_input_value = voice_prompt
js = f"""
    <script>
        function insertText(dummy_var_to_force_repeat_execution) {{
            var chatInput = parent.document.querySelector('textarea[data-testid="stChatInput"]');
            var nativeInputValueSetter = Object.getOwnPropertyDescriptor(window.HTMLTextAreaElement.prototype, "value").set;
            nativeInputValueSetter.call(chatInput, "{default_chat_input_value}");
            var event = new Event('input', {{ bubbles: true}});
            chatInput.dispatchEvent(event);
        }}
        insertText({len(st.session_state.messages)});
    </script>
    """
st.components.v1.html(js)    
